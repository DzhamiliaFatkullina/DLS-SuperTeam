{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df03207e",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dc6e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d9429",
   "metadata": {},
   "source": [
    "### Ranking of recommendation lists\n",
    "\n",
    "**`MetricsCalculator`** class is designed for evaluating our recommendation system's results with **Precision@k**, **Recall@k**, **MAP**, and **NDCG** metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCalculator:\n",
    "    @staticmethod\n",
    "    def precision_at_k(relevant_items: List[str], recommended_items: List[str], k: int) -> float:\n",
    "        top_k = recommended_items[:k]\n",
    "        relevant_in_top_k = len(set(top_k) & set(relevant_items))\n",
    "        return relevant_in_top_k / k if k > 0 else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def recall_at_k(relevant_items: List[str], recommended_items: List[str], k: int) -> float:\n",
    "        top_k = recommended_items[:k]\n",
    "        relevant_in_top_k = len(set(top_k) & set(relevant_items))\n",
    "        return relevant_in_top_k / len(relevant_items) if relevant_items else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def average_precision(relevant_items: List[str], recommended_items: List[str]) -> float:\n",
    "        ap = 0.0\n",
    "        num_relevant = len(relevant_items)\n",
    "        relevant_positions = [i+1 for i, item in enumerate(recommended_items) if item in relevant_items]\n",
    "        \n",
    "        for i, pos in enumerate(relevant_positions):\n",
    "            ap += (i+1) / pos\n",
    "        \n",
    "        return ap / num_relevant if num_relevant > 0 else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_average_precision(relevant_items_list: List[List[str]], recommended_items_list: List[List[str]]) -> float:\n",
    "        ap_scores = [\n",
    "            MetricsCalculator.average_precision(relevant, recommended)\n",
    "            for relevant, recommended in zip(relevant_items_list, recommended_items_list)\n",
    "        ]\n",
    "        return np.mean(ap_scores) if ap_scores else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def ndcg_at_k(relevant_items: List[str], recommended_items: List[str], k: int, relevance_scores: Dict[str, float] = None) -> float:\n",
    "        top_k = recommended_items[:k]\n",
    "        if relevance_scores is None:\n",
    "            relevance_scores = {item: 1.0 for item in relevant_items}\n",
    "        \n",
    "        dcg = sum(\n",
    "            (relevance_scores.get(item, 0) / np.log2(i + 2) \n",
    "            for i, item in enumerate(top_k))\n",
    "        )\n",
    "        \n",
    "        ideal_relevance = sorted([relevance_scores.get(item, 0) for item in relevant_items], reverse=True)[:k]\n",
    "        idcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(ideal_relevance))\n",
    "        \n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_all_metrics(\n",
    "        relevant_items_list: List[List[str]],\n",
    "        recommended_items_list: List[List[str]],\n",
    "        k_values: List[int] = [5],\n",
    "        relevance_scores_list: List[Dict[str, float]] = None\n",
    "    ) -> Dict[str, Union[float, Dict[int, float]]]:\n",
    "        results = {\n",
    "            'MAP': MetricsCalculator.mean_average_precision(relevant_items_list, recommended_items_list),\n",
    "            'Precision@k': {},\n",
    "            'Recall@k': {},\n",
    "            'NDCG@k': {}\n",
    "        }\n",
    "        \n",
    "        for k in k_values:\n",
    "            results['Precision@k'][k] = np.mean([\n",
    "                MetricsCalculator.precision_at_k(relevant, recommended, k)\n",
    "                for relevant, recommended in zip(relevant_items_list, recommended_items_list)\n",
    "            ])\n",
    "            \n",
    "            results['Recall@k'][k] = np.mean([\n",
    "                MetricsCalculator.recall_at_k(relevant, recommended, k)\n",
    "                for relevant, recommended in zip(relevant_items_list, recommended_items_list)\n",
    "            ])\n",
    "            \n",
    "            if relevance_scores_list:\n",
    "                results['NDCG@k'][k] = np.mean([\n",
    "                    MetricsCalculator.ndcg_at_k(relevant, recommended, k, rel_scores)\n",
    "                    for relevant, recommended, rel_scores in zip(relevant_items_list, recommended_items_list, relevance_scores_list)\n",
    "                ])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cabed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from rank_bm25 import BM25Okapi\n",
    "from annoy import AnnoyIndex\n",
    "import faiss\n",
    "\n",
    "\n",
    "class SearchEvaluator:\n",
    "    def __init__(self, data_path: str, dataset_path: str = \"final_dataset.csv\"):\n",
    "        # Загрузка данных валидации\n",
    "        with open(data_path, 'r') as f:\n",
    "            self.validation_data = json.load(f)\n",
    "        \n",
    "        # Загрузка датасета с фильмами\n",
    "        self.dataset = pd.read_csv(dataset_path)\n",
    "        self.movie_titles = self.dataset['title'].tolist()\n",
    "        \n",
    "        # Подготовка запросов и релевантных фильмов (храним только названия)\n",
    "        self.queries = [item['query'] for item in self.validation_data]\n",
    "        self.relevant_items_list = [item['relevant_movies'] for item in self.validation_data]\n",
    "        \n",
    "        # Загрузка эмбеддингов и инициализация моделей\n",
    "        self.embeddings = {}\n",
    "        self.models = {\n",
    "            'all_MiniLM_L12_v2': None,\n",
    "            'multi_qa_distilbert_cos_v1': None\n",
    "        }\n",
    "        \n",
    "        # Загружаем эмбеддинги и модели\n",
    "        for model_name in self.models:\n",
    "            emb_path = f\"embeddings/embeddings_{model_name.replace('-', '_')}.npy\"\n",
    "            self.embeddings[model_name] = np.load(emb_path)\n",
    "            self.models[model_name] = SentenceTransformer(model_name.replace(\"_\", \"-\"))\n",
    "            \n",
    "        # Инициализация BM25\n",
    "        tokenized_titles = [title.lower().split() for title in self.movie_titles]\n",
    "        self.bm25 = BM25Okapi(tokenized_titles)\n",
    "\n",
    "    def bm25_search(self, query: str, top_k: int = 5) -> List[str]:\n",
    "        tokenized_query = query.lower().split()\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        return [self.movie_titles[i] for i in top_indices]\n",
    "\n",
    "    def cosine_search(self, query: str, model_name: str, top_k: int = 5) -> List[str]:\n",
    "        model = self.models[model_name]\n",
    "        query_emb = model.encode([query])\n",
    "        emb_matrix = self.embeddings[model_name]\n",
    "        \n",
    "        # Нормализация и косинусная схожесть\n",
    "        query_emb = query_emb / np.linalg.norm(query_emb)\n",
    "        emb_matrix = emb_matrix / np.linalg.norm(emb_matrix, axis=1, keepdims=True)\n",
    "        scores = np.dot(emb_matrix, query_emb.T).flatten()\n",
    "        \n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        return [self.movie_titles[i] for i in top_indices]\n",
    "\n",
    "    def faiss_flat_search(self, query: str, model_name: str, top_k: int = 5) -> List[str]:\n",
    "        model = self.models[model_name]\n",
    "        query_emb = model.encode([query])\n",
    "        emb_matrix = self.embeddings[model_name]\n",
    "        \n",
    "        dimension = emb_matrix.shape[1]\n",
    "        index = faiss.IndexFlatIP(dimension)\n",
    "        index.add(emb_matrix.astype('float32'))\n",
    "        \n",
    "        _, indices = index.search(query_emb.astype('float32'), top_k)\n",
    "        return [self.movie_titles[i] for i in indices[0]]\n",
    "\n",
    "    def faiss_hnsw_search(self, query: str, model_name: str, top_k: int = 5) -> List[str]:\n",
    "        model = self.models[model_name]\n",
    "        query_emb = model.encode([query])\n",
    "        emb_matrix = self.embeddings[model_name]\n",
    "        \n",
    "        dimension = emb_matrix.shape[1]\n",
    "        index = faiss.IndexHNSWFlat(dimension, 32)\n",
    "        index.add(emb_matrix.astype('float32'))\n",
    "        \n",
    "        _, indices = index.search(query_emb.astype('float32'), top_k)\n",
    "        return [self.movie_titles[i] for i in indices[0]]\n",
    "\n",
    "    def annoy_search(self, query: str, model_name: str, top_k: int = 5) -> List[str]:\n",
    "        model = self.models[model_name]\n",
    "        query_emb = model.encode([query])\n",
    "        emb_matrix = self.embeddings[model_name]\n",
    "        \n",
    "        dimension = emb_matrix.shape[1]\n",
    "        annoy_index = AnnoyIndex(dimension, 'angular')\n",
    "        for i, emb in enumerate(emb_matrix):\n",
    "            annoy_index.add_item(i, emb)\n",
    "        annoy_index.build(10)\n",
    "        \n",
    "        indices = annoy_index.get_nns_by_vector(query_emb[0], top_k)\n",
    "        return [self.movie_titles[i] for i in indices]\n",
    "\n",
    "    def evaluate_all_approaches(self, output_file: str = \"results.json\"):\n",
    "        results = {}\n",
    "        search_approaches = {\n",
    "            'bm25': self.bm25_search,\n",
    "            'cosine': self.cosine_search,\n",
    "            'faiss_flat': self.faiss_flat_search,\n",
    "            'faiss_hnsw': self.faiss_hnsw_search,\n",
    "            'annoy': self.annoy_search\n",
    "        }\n",
    "        \n",
    "        for model_name in self.models:\n",
    "            results[model_name] = {}\n",
    "            \n",
    "            for approach_name, search_func in search_approaches.items():\n",
    "                if approach_name == 'bm25':\n",
    "                    # Для BM25 не используем модели\n",
    "                    print(f\"Evaluating BM25...\")\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    recommended_items_list = []\n",
    "                    for query in self.queries:\n",
    "                        recs = search_func(query, 5)\n",
    "                        recommended_items_list.append(recs)\n",
    "                else:\n",
    "                    print(f\"Evaluating {model_name} with {approach_name}...\")\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    recommended_items_list = []\n",
    "                    for query in self.queries:\n",
    "                        recs = search_func(query, model_name, 5)\n",
    "                        recommended_items_list.append(recs)\n",
    "                \n",
    "                # Вычисляем метрики, сравнивая только строки (названия)\n",
    "                metrics = MetricsCalculator.calculate_all_metrics(\n",
    "                    self.relevant_items_list,\n",
    "                    recommended_items_list,\n",
    "                    k_values=[5]  # We only care about top 5\n",
    "                )\n",
    "                \n",
    "                elapsed = time.time() - start_time\n",
    "                metrics['time_per_query'] = elapsed / len(self.queries)\n",
    "                \n",
    "                if approach_name == 'bm25':\n",
    "                    results['BM25'] = metrics\n",
    "                else:\n",
    "                    results[model_name][approach_name] = metrics\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6cfac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    evaluator = SearchEvaluator(\"validation_set.json\", \"final_dataset.csv\")\n",
    "    results = evaluator.evaluate_all_approaches()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
