{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df03207e",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3dc6e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d9429",
   "metadata": {},
   "source": [
    "### Ranking of recommendation lists\n",
    "\n",
    "**`MetricsCalculator`** class is designed for evaluating our recommendation system's results with **Precision@k**, **Recall@k**, **MAP**, and **NDCG** metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "72b6540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCalculator:\n",
    "    @staticmethod\n",
    "    def precision_at_k(relevant_items: List[str], recommended_items: List[str], k: int) -> float:\n",
    "        top_k = recommended_items[:k]\n",
    "        relevant_in_top_k = len(set(top_k) & set(relevant_items))\n",
    "        return relevant_in_top_k / k if k > 0 else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def recall_at_k(relevant_items: List[str], recommended_items: List[str], k: int) -> float:\n",
    "        top_k = recommended_items[:k]\n",
    "        relevant_in_top_k = len(set(top_k) & set(relevant_items))\n",
    "        return relevant_in_top_k / len(relevant_items) if relevant_items else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def average_precision(relevant_items: List[str], recommended_items: List[str]) -> float:\n",
    "        ap = 0.0\n",
    "        num_relevant = len(relevant_items)\n",
    "        relevant_positions = [i+1 for i, item in enumerate(recommended_items) if item in relevant_items]\n",
    "        \n",
    "        for i, pos in enumerate(relevant_positions):\n",
    "            ap += (i+1) / pos\n",
    "        \n",
    "        return ap / num_relevant if num_relevant > 0 else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_average_precision(relevant_items_list: List[List[str]], recommended_items_list: List[List[str]]) -> float:\n",
    "        ap_scores = [\n",
    "            MetricsCalculator.average_precision(relevant, recommended)\n",
    "            for relevant, recommended in zip(relevant_items_list, recommended_items_list)\n",
    "        ]\n",
    "        return np.mean(ap_scores) if ap_scores else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def ndcg_at_k(relevant_items: List[str], recommended_items: List[str], k: int, relevance_scores: Dict[str, float] = None) -> float:\n",
    "        top_k = recommended_items[:k]\n",
    "        if relevance_scores is None:\n",
    "            relevance_scores = {item: 1.0 for item in relevant_items}\n",
    "        \n",
    "        dcg = sum(\n",
    "            (relevance_scores.get(item, 0) / np.log2(i + 2) \n",
    "            for i, item in enumerate(top_k))\n",
    "        )\n",
    "        \n",
    "        ideal_relevance = sorted([relevance_scores.get(item, 0) for item in relevant_items], reverse=True)[:k]\n",
    "        idcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(ideal_relevance))\n",
    "        \n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_all_metrics(\n",
    "        relevant_items_list: List[List[str]],\n",
    "        recommended_items_list: List[List[str]],\n",
    "        k_values: List[int] = [5],\n",
    "        relevance_scores_list: List[Dict[str, float]] = None\n",
    "    ) -> Dict[str, Union[float, Dict[int, float]]]:\n",
    "        results = {\n",
    "            'MAP': MetricsCalculator.mean_average_precision(relevant_items_list, recommended_items_list),\n",
    "            'Precision@k': {},\n",
    "            'Recall@k': {},\n",
    "            'NDCG@k': {}\n",
    "        }\n",
    "        \n",
    "        for k in k_values:\n",
    "            results['Precision@k'][k] = np.mean([\n",
    "                MetricsCalculator.precision_at_k(relevant, recommended, k)\n",
    "                for relevant, recommended in zip(relevant_items_list, recommended_items_list)\n",
    "            ])\n",
    "            \n",
    "            results['Recall@k'][k] = np.mean([\n",
    "                MetricsCalculator.recall_at_k(relevant, recommended, k)\n",
    "                for relevant, recommended in zip(relevant_items_list, recommended_items_list)\n",
    "            ])\n",
    "            \n",
    "            if relevance_scores_list:\n",
    "                results['NDCG@k'][k] = np.mean([\n",
    "                    MetricsCalculator.ndcg_at_k(relevant, recommended, k, rel_scores)\n",
    "                    for relevant, recommended, rel_scores in zip(relevant_items_list, recommended_items_list, relevance_scores_list)\n",
    "                ])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cabed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "from annoy import AnnoyIndex\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "class SearchEvaluator:\n",
    "    def __init__(self, data_path: str, dataset_path: str = \"final_dataset.csv\"):\n",
    "        with open(data_path, 'r') as f:\n",
    "            self.validation_data = json.load(f)\n",
    "        \n",
    "        # Load movie dataset\n",
    "        self.dataset = pd.read_csv(dataset_path)\n",
    "        self.movie_titles = self.dataset['title'].tolist()\n",
    "        self.title_to_index = {title: idx for idx, title in enumerate(self.movie_titles)}\n",
    "        \n",
    "        # Prepare queries and relevant movies\n",
    "        self.queries = [item['query'] for item in self.validation_data]\n",
    "        self.relevant_items_list = [item['relevant_movies'] for item in self.validation_data]\n",
    "        \n",
    "        # Load embeddings and initialize models\n",
    "        self.embeddings = {}\n",
    "        self.models = {\n",
    "            'all_mpnet_base_v2': None,\n",
    "            'all_MiniLM_L12_v2': None,\n",
    "            'multi_qa_distilbert_cos_v1': None\n",
    "        }\n",
    "        \n",
    "        for model_name in self.models:\n",
    "            emb_path = f\"embeddings/embeddings_{model_name.replace('-', '_')}.npy\"\n",
    "            self.embeddings[model_name] = np.load(emb_path)\n",
    "            self.models[model_name] = SentenceTransformer(model_name.replace(\"_\", \"-\"))\n",
    "\n",
    "        # Initialize BM25\n",
    "        tokenized_titles = [title.lower().split() for title in self.movie_titles]\n",
    "        self.bm25 = BM25Okapi(tokenized_titles)\n",
    "\n",
    "    def _get_similarity_scores(self, query: str, movies: List[str], method: str, model_name: str = None) -> List[float]:\n",
    "        \"\"\"Get similarity scores for movies using the specified method\"\"\"\n",
    "        if method == 'bm25':\n",
    "            # TODO: починить\n",
    "            tokenized_query = query.lower().split()\n",
    "            return [self.bm25.get_scores(tokenized_query)[self.title_to_index[movie]] for movie in movies]\n",
    "        elif method in ['cosine', 'faiss_flat', 'faiss_hnsw', 'annoy']:\n",
    "            model = self.models[model_name]\n",
    "            query_emb = model.encode([query])\n",
    "            emb_matrix = self.embeddings[model_name]\n",
    "            \n",
    "            # Get embeddings for relevant movies\n",
    "            movie_indices = [self.title_to_index[movie] for movie in movies]\n",
    "            movie_embs = emb_matrix[movie_indices]\n",
    "            \n",
    "            # Normalize and compute cosine similarity\n",
    "            query_emb = query_emb / np.linalg.norm(query_emb)\n",
    "            movie_embs = movie_embs / np.linalg.norm(movie_embs, axis=1, keepdims=True)\n",
    "            return np.dot(movie_embs, query_emb.T).flatten().tolist()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    def _print_recommendations(self, query: str, recommendations: List[str], rec_scores: List[float],\n",
    "                             relevant_movies: List[str], method: str, model_name: str = None):\n",
    "        \"\"\"Print recommendations with scores and relevant movies with their scores\"\"\"\n",
    "        print(f\"\\nQuery: '{query}'\")\n",
    "        \n",
    "        # Get scores for relevant movies using the same metric\n",
    "        relevant_scores = self._get_similarity_scores(query, relevant_movies, method, model_name)\n",
    "        \n",
    "        print(\"\\nRelevant movies with scores:\")\n",
    "        for movie, score in zip(relevant_movies, relevant_scores):\n",
    "            print(f\"  - {movie} (score: {score:.4f})\")\n",
    "        \n",
    "        print(\"\\nRecommended movies with scores:\")\n",
    "        for rank, (movie, score) in enumerate(zip(recommendations, rec_scores), 1):\n",
    "            relevant_flag = \" [RELEVANT]\" if movie in relevant_movies else \"\"\n",
    "            print(f\"  {rank}. {movie} (score: {score:.4f}){relevant_flag}\")\n",
    "\n",
    "    def bm25_search(self, query: str, top_k: int = 5) -> Tuple[List[str], List[float]]:\n",
    "        tokenized_query = query.lower().split()\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        return [self.movie_titles[i] for i in top_indices], [scores[i] for i in top_indices]\n",
    "\n",
    "    def cosine_search(self, query: str, model_name: str, top_k: int = 5) -> Tuple[List[str], List[float]]:\n",
    "        model = self.models[model_name]\n",
    "        query_emb = model.encode([query])\n",
    "        emb_matrix = self.embeddings[model_name]\n",
    "        \n",
    "        query_emb = query_emb / np.linalg.norm(query_emb)\n",
    "        emb_matrix = emb_matrix / np.linalg.norm(emb_matrix, axis=1, keepdims=True)\n",
    "        scores = np.dot(emb_matrix, query_emb.T).flatten()\n",
    "        \n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        return [self.movie_titles[i] for i in top_indices], [scores[i] for i in top_indices]\n",
    "    \n",
    "    def faiss_flat_search(self, query: str, model_name: str, top_k: int = 5) -> Tuple[List[str], List[float]]:\n",
    "        model = self.models[model_name]\n",
    "        query_emb = model.encode([query])\n",
    "        emb_matrix = self.embeddings[model_name]\n",
    "        \n",
    "        dimension = emb_matrix.shape[1]\n",
    "        index = faiss.IndexFlatIP(dimension)\n",
    "        index.add(emb_matrix.astype('float32'))\n",
    "        \n",
    "        distances, indices = index.search(query_emb.astype('float32'), top_k)\n",
    "        return [self.movie_titles[i] for i in indices[0]], distances[0].tolist()\n",
    "\n",
    "    def faiss_hnsw_search(self, query: str, model_name: str, top_k: int = 5) -> Tuple[List[str], List[float]]:\n",
    "        model = self.models[model_name]\n",
    "        query_emb = model.encode([query])\n",
    "        emb_matrix = self.embeddings[model_name]\n",
    "        \n",
    "        dimension = emb_matrix.shape[1]\n",
    "        index = faiss.IndexHNSWFlat(dimension, 32)\n",
    "        index.add(emb_matrix.astype('float32'))\n",
    "        \n",
    "        distances, indices = index.search(query_emb.astype('float32'), top_k)\n",
    "        return [self.movie_titles[i] for i in indices[0]], distances[0].tolist()\n",
    "\n",
    "    def annoy_search(self, query: str, model_name: str, top_k: int = 5) -> Tuple[List[str], List[float]]:\n",
    "        model = self.models[model_name]\n",
    "        query_emb = model.encode([query])\n",
    "        emb_matrix = self.embeddings[model_name]\n",
    "        \n",
    "        dimension = emb_matrix.shape[1]\n",
    "        annoy_index = AnnoyIndex(dimension, 'angular')\n",
    "        for i, emb in enumerate(emb_matrix):\n",
    "            annoy_index.add_item(i, emb)\n",
    "        annoy_index.build(10)\n",
    "        \n",
    "        indices, distances = annoy_index.get_nns_by_vector(query_emb[0], top_k, include_distances=True)\n",
    "        # Convert angular distance to similarity score (higher is better)\n",
    "        scores = [(1 - (d**2) / 2) for d in distances]  # Convert angular distance to cosine similarity\n",
    "        return [self.movie_titles[i] for i in indices], scores\n",
    "\n",
    "    def evaluate_all_approaches(self, output_file: str = \"results.json\", show_examples: int = 3):\n",
    "        results = {}\n",
    "        search_approaches = {\n",
    "            'bm25': lambda q: self.bm25_search(q, 5),\n",
    "            'cosine': lambda q, m: self.cosine_search(q, m, 5),\n",
    "            'faiss_flat': lambda q, m: self.faiss_flat_search(q, m, 5),\n",
    "            'faiss_hnsw': lambda q, m: self.faiss_hnsw_search(q, m, 5),\n",
    "            'annoy': lambda q, m: self.annoy_search(q, m, 5)\n",
    "        }\n",
    "        \n",
    "        for model_name in self.models:\n",
    "            results[model_name] = {}\n",
    "            \n",
    "            for approach_name, search_func in search_approaches.items():\n",
    "                if approach_name == 'bm25':\n",
    "                    print(f\"\\n{'='*50}\\nEvaluating BM25...\\n{'='*50}\")\n",
    "                else:\n",
    "                    print(f\"\\n{'='*50}\\nEvaluating {model_name} with {approach_name}...\\n{'='*50}\")\n",
    "                \n",
    "                start_time = time.time()\n",
    "                recommended_items_list = []\n",
    "                rec_scores_list = []\n",
    "                \n",
    "                for i, (query, relevant) in enumerate(zip(self.queries, self.relevant_items_list)):\n",
    "                    if approach_name == 'bm25':\n",
    "                        recs, rec_scores = search_func(query)\n",
    "                    else:\n",
    "                        recs, rec_scores = search_func(query, model_name)\n",
    "\n",
    "                    recommended_items_list.append(recs)\n",
    "                    rec_scores_list.append(rec_scores)\n",
    "\n",
    "                    if i < show_examples:\n",
    "                        self._print_recommendations(\n",
    "                            query, recs, rec_scores, \n",
    "                            relevant, approach_name, model_name\n",
    "                        )\n",
    "\n",
    "                # Prepare relevance scores for NDCG\n",
    "                relevance_scores_list = []\n",
    "                for relevant, recs, rec_scores in zip(self.relevant_items_list, recommended_items_list, rec_scores_list):\n",
    "                    # Create dictionary with relevance scores (1 for relevant items, 0 for others)\n",
    "                    rel_scores = {item: 1.0 for item in relevant}\n",
    "                    relevance_scores_list.append(rel_scores)\n",
    "\n",
    "                # Calculate metrics\n",
    "                metrics = MetricsCalculator.calculate_all_metrics(\n",
    "                    self.relevant_items_list,\n",
    "                    recommended_items_list,\n",
    "                    [5],\n",
    "                    relevance_scores_list\n",
    "                )\n",
    "\n",
    "                elapsed = time.time() - start_time\n",
    "                metrics['time_per_query'] = elapsed / len(self.queries)\n",
    "\n",
    "                results[model_name][approach_name] = metrics\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6cfac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    evaluator = SearchEvaluator(\"validation_set.json\", \"final_dataset.csv\")\n",
    "    results = evaluator.evaluate_all_approaches(show_examples=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
