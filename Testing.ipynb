{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df03207e",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dc6e95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f10e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bb3cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genres_str'] = df['genres'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "combined_emb = np.load('movie_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e52842f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def recommend_movies(user_query, top_n=5):\n",
    "#     query_emb = model.encode([user_query], convert_to_tensor=True)\n",
    "#     query_emb = query_emb / np.linalg.norm(query_emb)\n",
    "    \n",
    "#     sim_scores = cosine_similarity(query_emb, combined_emb)\n",
    "#     top_indices = np.argsort(sim_scores[0])[-top_n:][::-1]\n",
    "    \n",
    "#     return df.iloc[top_indices]\n",
    "\n",
    "\n",
    "def recommend_movies(user_query, top_n=5, verbose=True):\n",
    "    query_emb = model.encode([user_query], convert_to_tensor=True)\n",
    "    query_emb = query_emb / np.linalg.norm(query_emb)\n",
    "    sim_scores = cosine_similarity(query_emb, combined_emb)\n",
    "    \n",
    "    top_indices = np.argsort(sim_scores[0])[-top_n:][::-1]\n",
    "    recommendations = df.iloc[top_indices].copy()\n",
    "    \n",
    "    recommendations['similarity_score'] = sim_scores[0][top_indices]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nРекомендации для запроса: '{user_query}'\\n\")\n",
    "        print(\"{:<5} {:<60} {:<15} {}\".format(\n",
    "            \"№\", \"Название фильма\", \"Схожесть\", \"Жанры\"))\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "        for i, (idx, row) in enumerate(recommendations.iterrows(), 1):\n",
    "            genres = ', '.join(row['genres']) if row['genres'] else 'нет данных'\n",
    "            print(\"{:<5} {:<60} {:<15.3f} {}\".format(\n",
    "                i, \n",
    "                row['title'][:55] + \"...\" if len(row['title']) > 55 else row['title'],\n",
    "                row['similarity_score'],\n",
    "                genres))\n",
    "    \n",
    "    return recommendations[['title', 'genres', 'similarity_score']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67e7de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Рекомендации для запроса: 'Crime drama about drug cartels and their violence'\n",
      "\n",
      "№     Название фильма                                              Схожесть        Жанры\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1     This Rebel Breed                                             0.595           [, ', c, r, i, m, e,  , d, r, a, m, a, ', ]\n",
      "2     Sinaloa Foot Soldier: Inside a Mexican Narco-Militia         0.574           [, ', ', ]\n",
      "3     Traffic                                                      0.573           [, ', r, e, a, l, i, s, m, ', ,,  , ', m, u, r, d, e, r, ', ,,  , ', v, i, o, l, e, n, c, e, ', ,,  , ', f, l, a, s, h, b, a, c, k, ', ,,  , ', h, u, m, o, r, ', ,,  , ', s, u, s, p, e, n, s, e, f, u, l, ', ]\n",
      "4     Buying Time                                                  0.572           [, ', c, r, i, m, e,  , d, r, a, m, a, ', ]\n",
      "5     Snow                                                         0.568           [, ', d, r, a, m, a, ', ]\n",
      "                                                   title  \\\n",
      "18152                                   This Rebel Breed   \n",
      "49863  Sinaloa Foot Soldier: Inside a Mexican Narco-M...   \n",
      "4873                                             Traffic   \n",
      "25920                                        Buying Time   \n",
      "42823                                               Snow   \n",
      "\n",
      "                                                  genres  similarity_score  \n",
      "18152                                    ['crime drama']          0.595460  \n",
      "49863                                               ['']          0.574174  \n",
      "4873   ['realism', 'murder', 'violence', 'flashback',...          0.573248  \n",
      "25920                                    ['crime drama']          0.571693  \n",
      "42823                                          ['drama']          0.567857  \n"
     ]
    }
   ],
   "source": [
    "user_query = \"Crime drama about drug cartels and their violence\"\n",
    "recommendations = recommend_movies(user_query)\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d9429",
   "metadata": {},
   "source": [
    "### Ranking of recommendation lists\n",
    "\n",
    "**`MetricsCalculator`** class is designed for evaluating our recommendation system's results with **Precision@k**, **Recall@k**, **MAP**, and **NDCG** metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72b6540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCalculator:\n",
    "    @staticmethod\n",
    "    def precision_at_k(relevant_items: List[int], recommended_items: List[int], k: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute Precision@k.\n",
    "        \n",
    "        Args:\n",
    "            relevant_items: List of ground-truth relevant item IDs.\n",
    "            recommended_items: Ranked list of recommended item IDs.\n",
    "            k: Number of top results to consider.\n",
    "        \n",
    "        Returns:\n",
    "            Precision@k score.\n",
    "        \"\"\"\n",
    "        top_k = recommended_items[:k]\n",
    "        relevant_in_top_k = len(set(top_k) & set(relevant_items))\n",
    "        return relevant_in_top_k / k if k > 0 else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def recall_at_k(relevant_items: List[int], recommended_items: List[int], k: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute Recall@k.\n",
    "        \n",
    "        Args:\n",
    "            relevant_items: List of ground-truth relevant item IDs.\n",
    "            recommended_items: Ranked list of recommended item IDs.\n",
    "            k: Number of top results to consider.\n",
    "        \n",
    "        Returns:\n",
    "            Recall@k score.\n",
    "        \"\"\"\n",
    "        top_k = recommended_items[:k]\n",
    "        relevant_in_top_k = len(set(top_k) & set(relevant_items))\n",
    "        return relevant_in_top_k / len(relevant_items) if relevant_items else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def average_precision(relevant_items: List[int], recommended_items: List[int]) -> float:\n",
    "        \"\"\"\n",
    "        Compute Average Precision (AP) for a single query.\n",
    "        \n",
    "        Args:\n",
    "            relevant_items: List of ground-truth relevant item IDs.\n",
    "            recommended_items: Ranked list of recommended item IDs.\n",
    "        \n",
    "        Returns:\n",
    "            AP score.\n",
    "        \"\"\"\n",
    "        ap = 0.0\n",
    "        num_relevant = len(relevant_items)\n",
    "        relevant_positions = [i+1 for i, item in enumerate(recommended_items) if item in relevant_items]\n",
    "        \n",
    "        for i, pos in enumerate(relevant_positions):\n",
    "            ap += (i+1) / pos\n",
    "        \n",
    "        return ap / num_relevant if num_relevant > 0 else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_average_precision(relevant_items_list: List[List[int]], recommended_items_list: List[List[int]]) -> float:\n",
    "        \"\"\"\n",
    "        Compute MAP (Mean Average Precision) across multiple queries.\n",
    "        \n",
    "        Args:\n",
    "            relevant_items_list: List of relevant item IDs for each query.\n",
    "            recommended_items_list: List of ranked recommendations for each query.\n",
    "        \n",
    "        Returns:\n",
    "            MAP score.\n",
    "        \"\"\"\n",
    "        ap_scores = [\n",
    "            MetricsCalculator.average_precision(relevant, recommended)\n",
    "            for relevant, recommended in zip(relevant_items_list, recommended_items_list)\n",
    "        ]\n",
    "        return np.mean(ap_scores) if ap_scores else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def ndcg_at_k(relevant_items: List[int], recommended_items: List[int], k: int, relevance_scores: Dict[int, float] = None) -> float:\n",
    "        \"\"\"\n",
    "        Compute NDCG@k.\n",
    "        \n",
    "        Args:\n",
    "            relevant_items: List of ground-truth relevant item IDs.\n",
    "            recommended_items: Ranked list of recommended item IDs.\n",
    "            k: Number of top results to consider.\n",
    "            relevance_scores: Dictionary of item relevance scores (default: binary relevance).\n",
    "        \n",
    "        Returns:\n",
    "            NDCG@k score.\n",
    "        \"\"\"\n",
    "        top_k = recommended_items[:k]\n",
    "        if relevance_scores is None:\n",
    "            relevance_scores = {item: 1.0 for item in relevant_items}\n",
    "        \n",
    "        dcg = sum(\n",
    "            (relevance_scores.get(item, 0) / np.log2(i + 2) \n",
    "            for i, item in enumerate(top_k))\n",
    "        )\n",
    "        \n",
    "        ideal_relevance = sorted([relevance_scores.get(item, 0) for item in relevant_items], reverse=True)[:k]\n",
    "        idcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(ideal_relevance))\n",
    "        \n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_all_metrics(\n",
    "        relevant_items_list: List[List[int]],\n",
    "        recommended_items_list: List[List[int]],\n",
    "        k_values: List[int] = [1, 3, 5, 10],\n",
    "        relevance_scores_list: List[Dict[int, float]] = None\n",
    "    ) -> Dict[str, Union[float, Dict[int, float]]]:\n",
    "        \"\"\"\n",
    "        Compute all metrics for a set of queries.\n",
    "        \n",
    "        Args:\n",
    "            relevant_items_list: List of relevant item IDs for each query.\n",
    "            recommended_items_list: List of ranked recommendations for each query.\n",
    "            k_values: List of k values for Precision@k and Recall@k.\n",
    "            relevance_scores_list: List of relevance score dictionaries for NDCG.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing all metrics:\n",
    "            - MAP\n",
    "            - Precision@k (for each k)\n",
    "            - Recall@k (for each k)\n",
    "            - NDCG@k (for each k, if relevance_scores provided).\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'MAP': MetricsCalculator.mean_average_precision(relevant_items_list, recommended_items_list),\n",
    "            'Precision@k': {},\n",
    "            'Recall@k': {},\n",
    "            'NDCG@k': {}\n",
    "        }\n",
    "        \n",
    "        for k in k_values:\n",
    "            results['Precision@k'][k] = np.mean([\n",
    "                MetricsCalculator.precision_at_k(relevant, recommended, k)\n",
    "                for relevant, recommended in zip(relevant_items_list, recommended_items_list)\n",
    "            ])\n",
    "            \n",
    "            results['Recall@k'][k] = np.mean([\n",
    "                MetricsCalculator.recall_at_k(relevant, recommended, k)\n",
    "                for relevant, recommended in zip(relevant_items_list, recommended_items_list)\n",
    "            ])\n",
    "            \n",
    "            if relevance_scores_list:\n",
    "                results['NDCG@k'][k] = np.mean([\n",
    "                    MetricsCalculator.ndcg_at_k(relevant, recommended, k, rel_scores)\n",
    "                    for relevant, recommended, rel_scores in zip(relevant_items_list, recommended_items_list, relevance_scores_list)\n",
    "                ])\n",
    "        \n",
    "        return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
